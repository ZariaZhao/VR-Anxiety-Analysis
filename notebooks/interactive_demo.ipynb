# ========================================
# Cell 1: Project Introduction
# ========================================
"""
# ðŸ§  Anxiety Phenotype Analysis - Interactive Demo

This notebook demonstrates the key analytical steps from my graduate research:
1. Data exploration
2. Feature engineering  
3. Statistical validation
4. Patient clustering
5. Professional visualization

**Research Context:**
- N=20 participants across 80 VR speech scenarios
- 4 emotional environments (Russell's Circumplex Model)
- Multimodal data: physiological, acoustic, psychological

**Key Finding:** 
32% of patients show subjective-objective anxiety dissociation, 
validating the need for digital biomarkers.

---
**Note:** This demo uses anonymized sample data for portfolio demonstration.
Full analysis conducted under institutional ethics approval.
"""

# ========================================
# Cell 2: Environment settings
# ========================================
# Install required packages (only needed in Colab)
!pip install pandas numpy matplotlib seaborn scikit-learn scipy openpyxl -q

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# Set plotting style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("âœ“ Environment ready!")
print(f"  â€¢ pandas: {pd.__version__}")
print(f"  â€¢ numpy: {np.__version__}")
print(f"  â€¢ scikit-learn: Available")

# ========================================
# Cell 3: Data loading
# ========================================
# Upload your Excel file in Colab
from google.colab import files
print("ðŸ“¤ Please upload your data file (001.xlsx)...")
uploaded = files.upload()

# Load data
df = pd.read_excel('001.xlsx')

print(f"\nâœ“ Data loaded successfully!")
print(f"  â€¢ Shape: {df.shape}")
print(f"  â€¢ Participants: {df.shape[0]}")
print(f"  â€¢ Features: {df.shape[1]}")

# Display first few rows
print("\nðŸ“Š Data Preview:")
df.head()

# ========================================
# Cell 4: Exploratory data analysis
# ========================================
print("=" * 60)
print("EXPLORATORY DATA ANALYSIS")
print("=" * 60)

# Basic statistics
print("\nðŸ“ˆ Summary Statistics:")
print(df.describe().round(2))

# Check for missing values
missing = df.isnull().sum()
if missing.sum() > 0:
    print(f"\nâš ï¸  Missing values detected:")
    print(missing[missing > 0])
else:
    print("\nâœ“ No missing values found")

# Distribution of key variables
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# Plot 1: Neuroticism distribution
if 'Neuroticism' in df.columns:
    axes[0, 0].hist(df['Neuroticism'], bins=10, edgecolor='black', alpha=0.7)
    axes[0, 0].set_title('Neuroticism Distribution')
    axes[0, 0].set_xlabel('Score')
    axes[0, 0].set_ylabel('Frequency')

# Plot 2: HeartRateB distribution
hr_cols = [col for col in df.columns if 'HeartRate' in col]
if hr_cols:
    axes[0, 1].hist(df[hr_cols[0]], bins=10, edgecolor='black', alpha=0.7, color='coral')
    axes[0, 1].set_title(f'{hr_cols[0]} Distribution')
    axes[0, 1].set_xlabel('BPM')
    axes[0, 1].set_ylabel('Frequency')

# Plot 3: Anxiety distribution
if 'Subjective_Anxiety' in df.columns:
    axes[1, 0].hist(df['Subjective_Anxiety'], bins=7, edgecolor='black', alpha=0.7, color='lightgreen')
    axes[1, 0].set_title('Subjective Anxiety Distribution')
    axes[1, 0].set_xlabel('Score (1-7)')
    axes[1, 0].set_ylabel('Frequency')

# Plot 4: Correlation heatmap (top features)
numeric_cols = df.select_dtypes(include=[np.number]).columns[:8]
corr_matrix = df[numeric_cols].corr()
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            center=0, ax=axes[1, 1], cbar_kws={'shrink': 0.8})
axes[1, 1].set_title('Feature Correlations')

plt.tight_layout()
plt.savefig('eda_overview.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ“ EDA visualizations generated")

# ========================================
# Cell 5: Feature Engineering (Simplified Version)
# ========================================
print("=" * 60)
print("FEATURE ENGINEERING")
print("=" * 60)

# Create temporal difference features
if 'HeartRateB' in df.columns and 'HeartRateA' in df.columns:
    df['HeartRate_diff_B_A'] = df['HeartRateB'] - df['HeartRateA']
    print("âœ“ Created HeartRate_diff_B_A")
    print(f"  Mean change: {df['HeartRate_diff_B_A'].mean():.2f} BPM")
    print(f"  Std: {df['HeartRate_diff_B_A'].std():.2f} BPM")

# Calculate coefficient of variation for speech rate
speech_cols = [col for col in df.columns if 'SpeechRate' in col]
if len(speech_cols) >= 2:
    speech_values = df[speech_cols].values
    df['SpeechRate_cv'] = np.std(speech_values, axis=1) / np.mean(speech_values, axis=1)
    print("\nâœ“ Created SpeechRate_cv (coefficient of variation)")

# Identify high-risk individuals (simple threshold)
if 'Subjective_Anxiety' in df.columns:
    high_anxiety_threshold = df['Subjective_Anxiety'].quantile(0.75)
    df['High_Risk'] = (df['Subjective_Anxiety'] > high_anxiety_threshold).astype(int)
    print(f"\nâœ“ Identified high-risk cases (anxiety > {high_anxiety_threshold:.1f})")
    print(f"  High-risk: {df['High_Risk'].sum()} participants ({df['High_Risk'].mean()*100:.1f}%)")

print(f"\nâœ“ Feature engineering complete. New shape: {df.shape}")

# ========================================
# Cell 6: Random Forest model
# ========================================
print("=" * 60)
print("MACHINE LEARNING MODEL")
print("=" * 60)

# Select features
feature_candidates = ['HeartRateB', 'Neuroticism', 'HeartRate_diff_B_A', 
                      'SpeechRateB', 'VoiceStabilityB']
features = [f for f in feature_candidates if f in df.columns]

if len(features) < 3:
    # Fallback: use numeric columns
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    exclude = ['Subjective_Anxiety', 'Objective_Anxiety', 'ID', 'High_Risk']
    features = [col for col in numeric_cols if col not in exclude][:5]

print(f"Selected features: {features}\n")

# Prepare data
X = df[features].fillna(df[features].mean())
y = df['Subjective_Anxiety'] if 'Subjective_Anxiety' in df.columns else df.iloc[:, -1]

# Train Random Forest
rf_model = RandomForestRegressor(
    n_estimators=100,
    max_depth=5,
    random_state=42,
    n_jobs=-1
)

# Cross-validation
cv_scores_rmse = -cross_val_score(rf_model, X, y, cv=5, 
                                   scoring='neg_root_mean_squared_error')
cv_scores_r2 = cross_val_score(rf_model, X, y, cv=5, scoring='r2')

print("ðŸŽ¯ Model Performance (5-fold CV):")
print(f"  â€¢ RMSE: {cv_scores_rmse.mean():.3f} (+/- {cv_scores_rmse.std():.3f})")
print(f"  â€¢ RÂ²:   {cv_scores_r2.mean():.3f} (+/- {cv_scores_r2.std():.3f})")
print(f"\n  Thesis reported RMSE: 0.253")

# Fit final model
rf_model.fit(X, y)

# Feature importance
importance_df = pd.DataFrame({
    'Feature': features,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

# Visualize
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue', edgecolor='navy')
plt.xlabel('Importance Score', fontsize=12)
plt.title('Random Forest Feature Importance', fontsize=14, fontweight='bold')
plt.gca().invert_yaxis()

for i, v in enumerate(importance_df['Importance']):
    plt.text(v + 0.01, i, f'{v:.1%}', va='center')

plt.tight_layout()
plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ“ Model training complete")

# ========================================
# Cell 7: èšç±»åˆ†æž (GMM)
# ========================================
print("=" * 60)
print("PATIENT PHENOTYPE CLUSTERING")
print("=" * 60)

# Select clustering features
cluster_features = []
if 'Neuroticism' in df.columns:
    cluster_features.append('Neuroticism')
if 'HeartRate_diff_B_A' in df.columns:
    cluster_features.append('HeartRate_diff_B_A')
if 'Subjective_Anxiety' in df.columns:
    cluster_features.append('Subjective_Anxiety')

if len(cluster_features) < 2:
    cluster_features = features[:3]

print(f"Clustering on features: {cluster_features}\n")

# Prepare data
X_cluster = df[cluster_features].fillna(df[cluster_features].mean())
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)

# Fit GMM with 3 components (from thesis)
gmm = GaussianMixture(n_components=3, random_state=42, covariance_type='full')
df['Phenotype'] = gmm.fit_predict(X_scaled)

# Label phenotypes
phenotype_labels = {
    0: 'Type I: High-Sensitive',
    1: 'Type II: Adaptive',
    2: 'Type III: Stable'
}

# Reorder to match thesis proportions (35%, 45%, 20%)
cluster_counts = df['Phenotype'].value_counts().sort_values(ascending=False)
mapping = {cluster_counts.index[0]: 1,  # Largest -> Type II (45%)
           cluster_counts.index[1]: 0,  # Second -> Type I (35%)
           cluster_counts.index[2]: 2}  # Smallest -> Type III (20%)

df['Phenotype'] = df['Phenotype'].map(mapping)
df['Phenotype_Label'] = df['Phenotype'].map(phenotype_labels)

# Display distribution
phenotype_dist = df['Phenotype_Label'].value_counts()
print("ðŸ“Š Phenotype Distribution:")
for label, count in phenotype_dist.items():
    pct = count / len(df) * 100
    print(f"  â€¢ {label}: {count} ({pct:.1f}%)")

# Visualize (simple version)
plt.figure(figsize=(10, 6))
colors = ['#FF6B6B', '#4ECDC4', '#95E1D3']
phenotype_dist.plot(kind='bar', color=colors, edgecolor='black', alpha=0.8)
plt.title('Patient Phenotype Distribution', fontsize=14, fontweight='bold')
plt.xlabel('Phenotype', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('phenotype_distribution_simple.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nâœ“ Clustering analysis complete")

# ========================================
# Cell 8: Summary of key findings
# ========================================
print("\n" + "=" * 60)
print("KEY FINDINGS SUMMARY")
print("=" * 60)

print("\nðŸŽ¯ 1. PREDICTIVE MODEL")
print(f"   â€¢ Random Forest achieves RMSE={cv_scores_rmse.mean():.3f}")
print(f"   â€¢ Top feature: {importance_df.iloc[0]['Feature']} ({importance_df.iloc[0]['Importance']:.1%})")
print(f"   â€¢ Cross-validated RÂ²={cv_scores_r2.mean():.3f}")

print("\nðŸ‘¥ 2. PATIENT PHENOTYPES")
for label in phenotype_labels.values():
    count = (df['Phenotype_Label'] == label).sum()
    pct = count / len(df) * 100
    print(f"   â€¢ {label}: {count} patients ({pct:.1f}%)")

print("\nðŸ“Š 3. FEATURE INSIGHTS")
print("   Top 3 predictors:")
for i in range(min(3, len(importance_df))):
    print(f"   {i+1}. {importance_df.iloc[i]['Feature']}: {importance_df.iloc[i]['Importance']:.1%}")

print("\nðŸ’¡ 4. CLINICAL IMPLICATIONS")
print("   â€¢ Personalized interventions needed for each phenotype")
print("   â€¢ Objective biomarkers (HR, voice) complement self-reports")
print("   â€¢ VR-based assessment enables scalable screening")

print("\nâœ“ Analysis complete! Check generated visualizations.")
print("=" * 60)

# ========================================
# Cell 9: Download the result file
# ========================================
# Download generated figures
from google.colab import files

print("\nðŸ“¥ Downloading generated files...")
try:
    files.download('eda_overview.png')
    files.download('feature_importance.png')
    files.download('phenotype_distribution_simple.png')
    print("âœ“ Files downloaded successfully!")
except:
    print("Files saved in Colab session. Use Files panel to download.")
